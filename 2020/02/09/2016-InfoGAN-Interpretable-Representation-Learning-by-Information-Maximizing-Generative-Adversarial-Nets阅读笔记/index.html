<!DOCTYPE html>

<html lang="zh">
    <head><meta name="generator" content="Hexo 3.8.0">
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <!--
        © SukkaW
        GitHub: https://github.com/SukkaW/hexo-theme-suka
        Version: 1.3.2
    -->
    <script>
window.lsVersion = "1.3.2",
window.oldVersion = [
    
        
            "0.2.0","0.0.1","0.1.0","1.0.0","1.0.1","1.1.0","1.1.1","1.2.0","1.3.0"
        
    
]
</script>

    <!-- ### DNS Prefetch ### -->
    <meta http-equiv="x-dns-prefetch-control" content="on">
<!-- busuanzi -->

    <link rel="dns-prefetch" href="//busuanzi.ibruce.info">


<!-- comment -->







<!-- analytics -->








    <!-- ### Preload ### -->
    
    <!-- Busuanzi -->
    
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/busuanzi@2.3.0/bsz.pure.mini.js" as="script">







    <!-- ### Meta & Title & Info ### -->
    <meta http-equiv="X-UA-Compatible" content="IE=Edge, chrome=1">
<meta name="renderer" content="webkit">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, viewport-fit=cover">

<!-- Title -->
<title>InfoGAN:Interpretable Representation Learning by Information MaximizingGenerative Adversarial Nets阅读笔记 | 幻华&#39;s Blog</title>

<!-- favicon -->
<!-- Favicons -->

    <link rel="shortcut icon" type="image/ico" href="/img/cbhq3-nfw26-006.ico">






<!-- Android Chrome Color -->



<meta name="format-detection" content="telephone=no">

<!-- Description -->
<meta name="description" content="

">

<!-- Keywords -->
<meta name="keywords" content=", 深度学习, 论文阅读">

<!-- Disable Fucking Bloody Baidu Tranformation -->
<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">

    <!-- ### Import File ### -->
    
        <!-- spectre.css -->

    <link rel="stylesheet" href="/lib/spectre/spectre.min.css">


<style>
    body {
        background-color: #f8f9fa;
    }

    a, a:visited {
        color: #0070ff;
    }

    a:active, a:focus, a:hover {
        color: #0070ff;
        opacity: .75;
    }

    #post-content a,
    #post-content a:hover,
    #post-content a:focus,
    #post-content a:visited {
        color: #005eb9;
        opacity: 1;
    }

    .post-entry .card-body a {
        color: #0070ff;
    }

    .avatar {
        background: #444;
    }

    .navbar-link,
    .navbar-link:visited,
    .timeline .timeline-item .timeline-icon.icon-lg {
        color: #0070ff;
    }

    .navbar-link:hover {
        color: #0070ff;
        opacity: .8;
    }

    #search-input .btn,
    #disqus_click_btn,
    #disqus-switch-to-direct,
    #disqus-loadmore-button {
        background: #727e96;
        border-color: #727e96;
        color: #fff;
    }

    #post-toc a.post-toc-link,
    #post-toc a.post-toc-link:visited,
    .share-menu.menu .menu-item>a {
        color: #727e96;
    }

    .share-menu.menu .menu-item>a:hover,
    .share-menu.menu .menu-item>a:focus,
    .share-menu.menu .menu-item>a:visited {
        color: #50596c;
        background: #f8f9fa;
        opacity: .85;
    }
</style>

<!-- style.css -->

    <link rel="stylesheet" href="/css/style.min.css">









    <!-- Prettify Theme -->
    
    
        <link rel="stylesheet" href="/css/highlight/github.min.css">
    



    

    <!-- ### Site Verification ### -->
    


    <!-- ### RSS ### -->
    
    
        <link rel="alternate" type="application/atom+xml" href="/atom.xml">
    


    <!-- ### WebApp ### -->
    <meta name="mobile-web-app-capable" content="yes">
<meta name="application-name" content="幻华&#39;s Blog">
<meta name="msapplication-starturl" content="http://www.zkhweb.top">

<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-title" content="幻华&#39;s Blog">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

<!-- Manifest Import -->

<!-- Open Search -->


    <!-- ### The Open Graph & Twitter Card Protocol ### -->
    <meta property="og:title" content="InfoGAN:Interpretable Representation Learning by Information MaximizingGenerative Adversarial Nets阅读笔记 | 幻华&#39;s Blog">
<meta property="og:site_name" content="幻华&#39;s Blog">

<meta property="og:locale" content="zh">

<meta property="og:url" content="http://www.zkhweb.top/2020/02/09/2016-InfoGAN-Interpretable-Representation-Learning-by-Information-Maximizing-Generative-Adversarial-Nets阅读笔记/">
<meta property="og:image" content="http://www.zkhweb.top/img/cbhq3-nfw26-006.ico">

<meta property="og:description" content="

">

<meta name="twitter:card" content="summary">


    <meta property="og:type" content="article">
    <meta property="article:published_time" content="2020-02-09T15:54:52.000Z">
    <meta property="article:modified_time" content="2020-02-09T15:54:52.000Z">
    <meta property="article:author" content="幻华(Zkher)">
    <meta property="og:article:tag" content=", 深度学习, 论文阅读"> 





    <!-- ### Analytics ### -->
    








    <!-- ### Canonical link ### -->
    <link rel="canonical" href="http://www.zkhweb.top/2020/02/09/2016-InfoGAN-Interpretable-Representation-Learning-by-Information-Maximizing-Generative-Adversarial-Nets阅读笔记/">

    <!-- ### Structured Data ### -->
    



<script type="application/ld+json">
{
    "@context": "http://schema.org",
    "url": "http://www.zkhweb.top/2020/02/09/2016-InfoGAN-Interpretable-Representation-Learning-by-Information-Maximizing-Generative-Adversarial-Nets阅读笔记/",
    "@type": "BlogPosting",
    "logo": "http://www.zkhweb.top/img/cbhq3-nfw26-006.ico",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://www.zkhweb.top/2020/02/09/2016-InfoGAN-Interpretable-Representation-Learning-by-Information-Maximizing-Generative-Adversarial-Nets阅读笔记/"
    },
    "headline": "InfoGAN:Interpretable Representation Learning by Information MaximizingGenerative Adversarial Nets阅读笔记 | 幻华&#39;s Blog",
    
    "image": {
        "@type": "ImageObject",
        "url": "http://www.zkhweb.top/img/cbhq3-nfw26-006.ico"
    },
    
    "datePublished": "2020-02-09T15:54:52.000Z",
    "dateModified": "2020-02-12T15:21:53.901Z",
    "author": {
        "@type": "Person",
        "name": "幻华(Zkher)",
        "image": {
            "@type": "ImageObject",
            "url": "http://www.zkhweb.top/img/my.jpg"
        },
        "description": "Hi, nice to meet you."
    },
    "publisher": {
        "@type": "Organization",
        "name": "幻华&#39;s Blog",
        "logo": {
            "@type": "ImageObject",
            "url": "http://www.zkhweb.top/img/cbhq3-nfw26-006.ico"
        }
    },
    
    "potentialAction": {
        "@type": "SearchAction",
        "target": "http://www.zkhweb.top/search/?s={search_term_string}",
        "query-input": "required name=search_term_string"
    },
    
    "keywords": ", 深度学习, 论文阅读",
    "description": "

"
}
</script>



    <!-- ### Custom Head ### --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    


</head>

    <body>
        

            

            <!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- ### Main content ### -->
            <!-- ## Header ##-->
<header>
    <h1 class="header-title text-center"><a href="/">幻华&#39;s Blog</a></h1>

    <p class="text-center header-slogan">
        
            
                Hi, nice to meet you.
            
        
    </p>

    <nav class="navbar-section text-center">
    
        <a href="/" class="navbar-link">首页</a>
    
    
        <a href="/archives/" class="navbar-link">归档</a>
    
    
        <a href="/search/" class="navbar-link">搜索</a>
    
    
        <a href="/about/" class="navbar-link">关于</a>
    
        <a href="/tag/" class="navbar-link">标签</a>
    
        <a href="/gallery/" class="navbar-link">画廊</a>
    
        <a href="/links/" class="navbar-link">友链</a>
    
        <a href="/en/" class="navbar-link">英语</a>
    
    
        <div class="dropdown dropdown-right">
    <a class="navbar-link dropdown-toggle" tabindex="0">分享</a>
    <ul class="menu share-menu">

        <!-- Share Weibo -->
        
        <li class="menu-item">
            <a href="http://service.weibo.com/share/share.php?appkey=&title=幻华's Blog&url=http://www.zkhweb.top&pic=http://www.zkhweb.top/img/cbhq3-nfw26-006.ico&searchPic=false&style=simple" target="_blank" rel="noopener noreferrer nofollow">分享到微博</a>
        </li>
        

        <!-- Share Twitter -->
        
        <li class="menu-item">
            <a href="https://twitter.com/intent/tweet?text=幻华's Blog&url=http://www.zkhweb.top&via=幻华(Zkher)" target="_blank" rel="noopener noreferrer nofollow">分享到 Twitter</a>
        </li>
        

        <!-- Share Facebook -->
        
        <li class="menu-item">
            <a href="https://www.facebook.com/sharer/sharer.php?u=http://www.zkhweb.top" target="_blank" rel="noopener noreferrer nofollow">分享到 Facebook</a>
        </li>
        

        <!-- Share Google+ -->
        
        <li class="menu-item">
            <a href="https://plus.google.com/share?url=http://www.zkhweb.top" target="_blank" rel="noopener noreferrer nofollow">分享到 Google+</a>
        </li>
        

        <!-- Share LinkedIn -->
        
        <li class="menu-item">
            <a href="https://www.linkedin.com/shareArticle?mini=true&url=http://www.zkhweb.top&title=InfoGAN:Interpretable Representation Learning by Information MaximizingGenerative Adversarial Nets阅读笔记" target="_blank" rel="noopener noreferrer nofollow">分享到 LinkedIn</a>
        </li>
        

        <!-- Share QQ -->
        
        <li class="menu-item">
            <a href="http://connect.qq.com/widget/shareqq/index.html?site=幻华's Blog&title=InfoGAN:Interpretable Representation Learning by Information MaximizingGenerative Adversarial Nets阅读笔记&summary=&pics=http://www.zkhweb.top/img/cbhq3-nfw26-006.ico&url=http://www.zkhweb.top" target="_blank" rel="noopener noreferrer nofollow"> 分享到 QQ</a>
        </li>
        

        <!-- Share Telegram -->
        
        <li class="menu-item">
            <a href="https://t.me/share/url?url=http://www.zkhweb.top&text=InfoGAN:Interpretable Representation Learning by Information MaximizingGenerative Adversarial Nets阅读笔记" target="_blank" rel="noopener noreferrer nofollow">分享到 Telegram</a>
        </li>
        

        <!-- QRCode -->
        
        <li class="menu-item">
            <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAANQAAADUCAAAAADBtVLEAAADpUlEQVR42u3a21LrQAxE0fz/T8NroDzqrYmNhWvn5RwIGXulai5u6fX1wNdLlChRokQ9DvUqXj8+8Pa79/d+/937744+sxp/9d7R51b3KUrUFNThpPs18NHPR/Dq7xN09UXj+xQlahBqNchyUhbI6gbSOGlxqq4lStR/QaUF4miyk8ldIVZgUaKeglptnqvJTw/IqxtPh2RRoqajyCArUGcjrb6U9PnTTumiRF2Mqib2pH9PS5NEiboQFUP35oNamuhpQVgdjk+teogSdSGKboQ7C0oaL32BdKMWJWoaqgpb0qEyHUhpcY1ep1XIFiXqJlQMNED43ymMdwvWpIAgStQUFDnAVmF9ezI3NmLyf1GipqFS4JFAKfxPh9DO4be6pihRU1B0o62apsirU1jrBECiRE1E7QT17UkMHxpTA3HrRCFK1I2oTjG6ugAJQ6vAhhwAYiFblKgbUTRs3Akq0+RPTVidEEaUqEmoTnBZTeIqnEwPoqkInh40RYmahkoFgnTITEUAUhQg1ygbTUSJGoCigT+9oWrhoA+V3U1XlKhpqFTQps1VqfGKXLO7QeM0SZSoP0R1Cm7dht1OMY0EPuVhW5SoAShaMNvZZLvNVyT0LMcSJWoAikzaBKShf5rwpDBQFuFEiRqAogXsOGAIcEgwSgpw5RcnStQAFD1c0uYPGvKnhsYUyny8+okSdRGKNvVWA1fFO7ohk+CmetAUJWoSKhXGyA2npq2dgzHd8EWJmoTqNCG2GjVA4frTB8nyQCtK1I0o0pRYNmU0N2e6sdNDsChR01BpkpMLdTZiesOdTVqUqEkoOnA3YEwH406gs7X6iRJ1A4o2bqTAv9tgkjb1TuFblKiJqM7Bs7Px0odJMm4sQogSNQhFQ0zabNgtwJFAFR9oRYm6EdV5fXL43NnMq4dVUaKmoshgaaGoDq9kUSENwagAIUrUAFQKDlPgTx76yIZMFpw0jihRU1CkoEwXA9oAHBs9QONVLBCIEnUjijTrJmSnmSotTKjBSpSogahPi2FpEtNQJ4WZaUxRoqagaNhCA/9OwxU6pMLAVJSoKSg6CdMNkPAyBZdVoY0U4kSJmoJK4UccLBw0U5iy82XFhUKUqJtRaYKSQgEJSs4+0JYLhShRg1FpM0w3RwtpNMAUJeo/o8gEToWzVBjvvHfK5itK1MUossGSJqgU2pCAp/P+1kOiKFEXozqNHmcUEToNJykwFSVqGupJL1GiRIkS9ZjXN27GEl7ke4mgAAAAAElFTkSuQmCC" alr="QRCode">
        </li>
        

    </ul>
</div>
    
    
        <a href="/atom.xml" target="_blank" class="navbar-link">RSS</a>
    
</nav>
</header>

            
    <!-- ## Post ## -->
    
    


<div class="post-container">
    <div id="post-card" class="card">
        
        <div class="card-item-container">
            <div class="card-inner-cell">
                <!-- # Post Header Info # -->
                <div class="card-header">
                    
    <h1 class="card-title h3 mb-2">InfoGAN:Interpretable Representation Learning by Information MaximizingGenerative Adversarial Nets阅读笔记</h1>




<div class="post-header-info">
    <p class="post-header-info-left text-gray">
        <img class="author-thumb lazyload" data-src="/img/my.jpg" src="/img/suka-lazyload.gif" alt="幻华(Zkher)'s Avatar">
        <span>2020-02-09</span>
        
            <span class="suka-devide-dot"></span>
            <a class="category-link" href="/categories/深度学习论文阅读/">深度学习论文阅读</a>
        
        
            <!-- Busuanzi Post Views -->
<span id="busuanzi_container_page_pv" hidden>
    <span class="suka-devide-dot"></span>
    <span></span>
    <span id="busuanzi_value_page_pv"></span>
    <span>Views</span>
</span>
        
        
    </p>
    <div class="post-header-info-right">
        
            <div class="dropdown dropdown-right">
<a class="dropdown-toggle" tabindex="0">分享本文</a>
<ul class="menu share-menu">
    <!-- Share Weibo -->
    
    <li class="menu-item">
        <a href="http://service.weibo.com/share/share.php?appkey=&title=InfoGAN:Interpretable Representation Learning by Information MaximizingGenerative Adversarial Nets阅读笔记&url=http://www.zkhweb.top/2020/02/09/2016-InfoGAN-Interpretable-Representation-Learning-by-Information-Maximizing-Generative-Adversarial-Nets阅读笔记/&pic=http://www.zkhweb.top/img/cbhq3-nfw26-006.ico&searchPic=false&style=simple" target="_blank" rel="noopener noreferrer nofollow">分享到微博</a>
    </li>
    

    <!-- Share Twitter -->
    
    <li class="menu-item">
        <a href="https://twitter.com/intent/tweet?text=InfoGAN:Interpretable Representation Learning by Information MaximizingGenerative Adversarial Nets阅读笔记&url=http://www.zkhweb.top/2020/02/09/2016-InfoGAN-Interpretable-Representation-Learning-by-Information-Maximizing-Generative-Adversarial-Nets阅读笔记/&via=幻华(Zkher)" target="_blank" rel="noopener noreferrer nofollow">分享到 Twitter</a>
    </li>
    

    <!-- Share Facebook -->
    
    <li class="menu-item">
        <a href="https://www.facebook.com/sharer/sharer.php?u=http://www.zkhweb.top/2020/02/09/2016-InfoGAN-Interpretable-Representation-Learning-by-Information-Maximizing-Generative-Adversarial-Nets阅读笔记/" target="_blank" rel="noopener noreferrer nofollow">分享到 Facebook</a>
    </li>
    

    <!-- Share Google+ -->
    
    <li class="menu-item">
        <a href="https://plus.google.com/share?url=http://www.zkhweb.top/2020/02/09/2016-InfoGAN-Interpretable-Representation-Learning-by-Information-Maximizing-Generative-Adversarial-Nets阅读笔记/" target="_blank" rel="noopener noreferrer nofollow">分享到 Google+</a>
    </li>
    

    <!-- Share LinkedIn -->
    
    <li class="menu-item">
        <a href="https://www.linkedin.com/shareArticle?mini=true&url=http://www.zkhweb.top/2020/02/09/2016-InfoGAN-Interpretable-Representation-Learning-by-Information-Maximizing-Generative-Adversarial-Nets阅读笔记/&title=幻华's Blog" target="_blank" rel="noopener noreferrer nofollow">分享到 LinkedIn</a>
    </li>
    

    <!-- Share QQ -->
    
    <li class="menu-item">
        <a href="http://connect.qq.com/widget/shareqq/index.html?site=幻华's Blog&title=幻华's Blog&summary=&pics=http://www.zkhweb.top/img/cbhq3-nfw26-006.ico&url=http://www.zkhweb.top/2020/02/09/2016-InfoGAN-Interpretable-Representation-Learning-by-Information-Maximizing-Generative-Adversarial-Nets阅读笔记/" target="_blank" rel="noopener noreferrer nofollow"> 分享到 QQ</a>
    </li>
    

    <!-- Share Telegram -->
    
    <li class="menu-item">
        <a href="https://t.me/share/url?url=http://www.zkhweb.top/2020/02/09/2016-InfoGAN-Interpretable-Representation-Learning-by-Information-Maximizing-Generative-Adversarial-Nets阅读笔记/&text=幻华's Blog" target="_blank" rel="noopener noreferrer nofollow">分享到 Telegram</a>
    </li>
    

    <!-- QRCode -->
    
    <li class="menu-item">
        <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAANQAAADUCAAAAADBtVLEAAADpUlEQVR42u3a21LrQAxE0fz/T8NroDzqrYmNhWvn5RwIGXulai5u6fX1wNdLlChRokQ9DvUqXj8+8Pa79/d+/937744+sxp/9d7R51b3KUrUFNThpPs18NHPR/Dq7xN09UXj+xQlahBqNchyUhbI6gbSOGlxqq4lStR/QaUF4miyk8ldIVZgUaKeglptnqvJTw/IqxtPh2RRoqajyCArUGcjrb6U9PnTTumiRF2Mqib2pH9PS5NEiboQFUP35oNamuhpQVgdjk+teogSdSGKboQ7C0oaL32BdKMWJWoaqgpb0qEyHUhpcY1ep1XIFiXqJlQMNED43ymMdwvWpIAgStQUFDnAVmF9ezI3NmLyf1GipqFS4JFAKfxPh9DO4be6pihRU1B0o62apsirU1jrBECiRE1E7QT17UkMHxpTA3HrRCFK1I2oTjG6ugAJQ6vAhhwAYiFblKgbUTRs3Akq0+RPTVidEEaUqEmoTnBZTeIqnEwPoqkInh40RYmahkoFgnTITEUAUhQg1ygbTUSJGoCigT+9oWrhoA+V3U1XlKhpqFTQps1VqfGKXLO7QeM0SZSoP0R1Cm7dht1OMY0EPuVhW5SoAShaMNvZZLvNVyT0LMcSJWoAikzaBKShf5rwpDBQFuFEiRqAogXsOGAIcEgwSgpw5RcnStQAFD1c0uYPGvKnhsYUyny8+okSdRGKNvVWA1fFO7ohk+CmetAUJWoSKhXGyA2npq2dgzHd8EWJmoTqNCG2GjVA4frTB8nyQCtK1I0o0pRYNmU0N2e6sdNDsChR01BpkpMLdTZiesOdTVqUqEkoOnA3YEwH406gs7X6iRJ1A4o2bqTAv9tgkjb1TuFblKiJqM7Bs7Px0odJMm4sQogSNQhFQ0zabNgtwJFAFR9oRYm6EdV5fXL43NnMq4dVUaKmoshgaaGoDq9kUSENwagAIUrUAFQKDlPgTx76yIZMFpw0jihRU1CkoEwXA9oAHBs9QONVLBCIEnUjijTrJmSnmSotTKjBSpSogahPi2FpEtNQJ4WZaUxRoqagaNhCA/9OwxU6pMLAVJSoKSg6CdMNkPAyBZdVoY0U4kSJmoJK4UccLBw0U5iy82XFhUKUqJtRaYKSQgEJSs4+0JYLhShRg1FpM0w3RwtpNMAUJeo/o8gEToWzVBjvvHfK5itK1MUossGSJqgU2pCAp/P+1kOiKFEXozqNHmcUEToNJykwFSVqGupJL1GiRIkS9ZjXN27GEl7ke4mgAAAAAElFTkSuQmCC" alt="QRCode">
    </li>
    

</ul>
</div>
        
    </div>
</div>
                </div>
                <div class="card-body">
                    
                        
                            <div id="post-toc">
                                <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#undefined"><span class="post-toc-number">1.</span> <span class="post-toc-text">1. InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#undefined"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">1.1. 摘要</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#undefined"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">1.2. 引言</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#undefined"><span class="post-toc-number">1.3.</span> <span class="post-toc-text">1.3. 相关工作</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#undefined"><span class="post-toc-number">1.4.</span> <span class="post-toc-text">1.4. 补充知识</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#undefined"><span class="post-toc-number">1.4.1.</span> <span class="post-toc-text">1.4.1. 信息量</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#undefined"><span class="post-toc-number">1.4.2.</span> <span class="post-toc-text">1.4.2. 信息熵</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#undefined"><span class="post-toc-number">1.4.3.</span> <span class="post-toc-text">1.4.3. 互信息</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#undefined"><span class="post-toc-number">1.5.</span> <span class="post-toc-text">1.5. InfoGAN</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#undefined"><span class="post-toc-number">1.5.1.</span> <span class="post-toc-text">1.5.1. 知识回顾——生成对抗网络</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#undefined"><span class="post-toc-number">1.5.2.</span> <span class="post-toc-text">1.5.2. 符号定义</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#undefined"><span class="post-toc-number">1.5.3.</span> <span class="post-toc-text">1.5.3. 直观感受</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#undefined"><span class="post-toc-number">1.5.4.</span> <span class="post-toc-text">1.5.4. 目标函数</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#undefined"><span class="post-toc-number">1.5.5.</span> <span class="post-toc-text">1.5.5. 对抗</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#undefined"><span class="post-toc-number">1.5.6.</span> <span class="post-toc-text">1.5.6. 目标函数由来</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#undefined"><span class="post-toc-number">1.5.7.</span> <span class="post-toc-text">1.5.7. 变分互信息最大化</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#undefined"><span class="post-toc-number">1.6.</span> <span class="post-toc-text">1.6. 实现</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#undefined"><span class="post-toc-number">1.7.</span> <span class="post-toc-text">1.7. 实验</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#undefined"><span class="post-toc-number">1.7.1.</span> <span class="post-toc-text">1.7.1. 互信息的最大化</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#undefined"><span class="post-toc-number">1.7.2.</span> <span class="post-toc-text">1.7.2. 有区分度的表示</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#undefined"><span class="post-toc-number">1.8.</span> <span class="post-toc-text">1.8. 结论</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#undefined"><span class="post-toc-number">1.9.</span> <span class="post-toc-text">1.9. 附录——解释为“sleep-sleep”算法</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#undefined"><span class="post-toc-number">1.10.</span> <span class="post-toc-text">1.10. 参考资料</span></a></li></ol></li></ol>
                            </div>
                        
                    
                    <article id="post-content">
                        <!-- toc -->
<ul>
<li><a href="#1-infogan-interpretable-representation-learning-by-information-maximizing-generative-adversarial-nets">1. InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</a><ul>
<li><a href="#11-摘要">1.1. 摘要</a></li>
<li><a href="#12-引言">1.2. 引言</a></li>
<li><a href="#13-相关工作">1.3. 相关工作</a></li>
<li><a href="#14-补充知识">1.4. 补充知识</a><ul>
<li><a href="#141-信息量">1.4.1. 信息量</a></li>
<li><a href="#142-信息熵">1.4.2. 信息熵</a></li>
<li><a href="#143-互信息">1.4.3. 互信息</a></li>
</ul>
</li>
<li><a href="#15-infogan">1.5. InfoGAN</a><ul>
<li><a href="#151-知识回顾生成对抗网络">1.5.1. 知识回顾——生成对抗网络</a></li>
<li><a href="#152-符号定义">1.5.2. 符号定义</a></li>
<li><a href="#153-直观感受">1.5.3. 直观感受</a></li>
<li><a href="#154-目标函数">1.5.4. 目标函数</a></li>
<li><a href="#155-对抗">1.5.5. 对抗</a></li>
<li><a href="#156-目标函数由来">1.5.6. 目标函数由来</a></li>
<li><a href="#157-变分互信息最大化">1.5.7. 变分互信息最大化</a></li>
</ul>
</li>
<li><a href="#16-实现">1.6. 实现</a></li>
<li><a href="#17-实验">1.7. 实验</a><ul>
<li><a href="#171-互信息的最大化">1.7.1. 互信息的最大化</a></li>
<li><a href="#172-有区分度的表示">1.7.2. 有区分度的表示</a></li>
</ul>
</li>
<li><a href="#18-结论">1.8. 结论</a></li>
<li><a href="#19-附录解释为sleep-sleep算法">1.9. 附录——解释为“sleep-sleep”算法</a></li>
<li><a href="#110-参考资料">1.10. 参考资料</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->
<hr>
<h1><span id="1-infogan-interpretable-representation-learning-by-information-maximizing-generative-adversarial-nets">1. InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</span></h1><blockquote>
<p>arXiv:1606.03657 [cs.LG]<br>tensorflow2代码：<a href="https://github.com/zhangkaihua88/ML_Paper" target="_blank" rel="noopener">https://github.com/zhangkaihua88/ML_Paper</a></p>
</blockquote>
<hr>
<h2><span id="11-摘要">1.1. 摘要</span></h2><p><strong>InfoGAN</strong></p>
<ul>
<li>对生成对抗网络的信息理论扩展</li>
<li>完全无监督的方式学习特征分离表示</li>
<li>能最大化潜在变量的一小部分与观察(生成)结果之间的互信息</li>
</ul>
<p><strong>本文</strong></p>
<ul>
<li>有效优化的互信息目标的下界</li>
<li>InfoGAN可以学习与现有监督方法学习得到了具有竞争力的可解释性表征。</li>
</ul>
<hr>
<h2><span id="12-引言">1.2. 引言</span></h2><p><strong>无监督学习</strong></p>
<ul>
<li>一般可以被描述为从大量存在的未标记数据中提取数值的问题。</li>
<li>流行框架是<strong>表征学习</strong><ul>
<li>目标是使用未标记的数据来学习一种表示，以从重要语义特征中找到易于解释的要素。</li>
</ul>
</li>
<li>特征分离的表示对于需要知道数据的显著属性的自然任务可能是有用的</li>
<li>无监督学习算法必须对下游分类任务有正确的效果（在不直接接触下游任务的情况下）。</li>
<li>很大一部分是由生成模型驱动的。<ul>
<li>动力源于对生成模型能力的相信，或为观察数据“创造”某种形式的理解，并希望良好的生成模型能自动学习一个有区分度的表示，即便通过随便一个不好的表示也容易构造完美的生成模型。最突出的生成模型是变分自动编码器（VAE）和生成对抗网络（GAN）。</li>
</ul>
</li>
</ul>
<p><strong>本文</strong></p>
<ul>
<li>生成对抗网络目标进行了简单的修改，鼓励其学习可解释和有意义的表示。</li>
<li>通过最大化GAN噪声变量的固定小子集与观测值之间的互信息来实现，这一点相对比较直观。</li>
<li>增加互信息成本的生成模型是学习特征表示的有效途径。</li>
</ul>
<hr>
<h2><span id="13-相关工作">1.3. 相关工作</span></h2><p>现在存在大量关于无监督表示学习的工作。</p>
<ul>
<li>早期的方法是基于堆叠的（通常是去噪）自动编码器或受限玻尔兹曼机</li>
<li>阶梯网络，它在MNIST数据集的半监督变学习上取得了惊人的成果。</li>
</ul>
<p>此外，先前的研究试图使用监督数据来学习分离的特征表示。</p>
<ul>
<li>使用监督学习训练表示的一部分来匹配所提供的标签</li>
<li>弱监督方法来消除对明确标记变量的需要</li>
</ul>
<hr>
<h2><span id="14-补充知识">1.4. 补充知识</span></h2><h3><span id="141-信息量">1.4.1. 信息量</span></h3><p>$ I(x) = -\log {p(x)} = \log { \frac { 1}{ p (x) }  } $<br>一个事件发生的概率越大，这件事情发生所包含的信息量就越小，比如说一个高富帅追求一个白富美，追到手了没有什么稀奇的，因为这是一件概率很高的事情，但是如果一个矮穷矬追求一个白富美，追到手了，这种事情发生的概率很低，其中一定有其他的原因：比如这个矮穷矬救过白富美的命或者这个矮穷矬器大活好不黏人，所以概率低的事情发生所包含的信息量大；两个相互独立的事情同时发生的信息量是两者单独发生的信息量之和。</p>
<h3><span id="142-信息熵">1.4.2. 信息熵</span></h3><p>信息量的均值</p>
<script type="math/tex; mode=display">H(x) = - \sum _{ x } p(x)log p(x)</script><h3><span id="143-互信息">1.4.3. 互信息</span></h3><p>在信息论中，X和Y之间的互信息 $I(X;Y)$测量从随机变量Y的知识中学习的关于另一个随机变量X的“信息量”。</p>
<ul>
<li>互信息可以表示为两个熵项的差值：<script type="math/tex; mode=display">I(X;Y)=H(X)-H(X|Y)=H(Y)-H(Y|X)</script>这个定义有一个直观的解释： $I(X,Y)$是观察到 $Y$ 时， $X$的不确定性的减少量。如果 $X$ 和 $Y$ 是独立的，那么 $I(X;Y)=0$ ，因为一个变量与另一个变量毫无关系；相反，如果$X$和$Y$通过确定性可逆函数相关，则获得最大互信息。<br><img src="https://image.zkhweb.top/20200209214308.png" alt="20200209214308.png"></li>
</ul>
<hr>
<h2><span id="15-infogan">1.5. InfoGAN</span></h2><h3><span id="151-知识回顾生成对抗网络">1.5.1. 知识回顾——生成对抗网络</span></h3><p>生成器G，判别器D，相互对抗使目标函数，达到最优。</p>
<script type="math/tex; mode=display">\min _{G}\max _{ D } V(D,G)={ \mathbb{E} }_{ x ～ { p }_  { data } (x) }[logD(x)] + { \mathbb{E} }_{ z ～ { p }_{ z }(z) }[log(1-D(G(z)))]</script><p>但是<strong>无约束、不可控、噪声信号z很难解释等问题。</strong></p>
<h3><span id="152-符号定义">1.5.2. 符号定义</span></h3><p>$x$ $\rightarrow$ 真实数据<br>$y$ $\rightarrow$ 标签（辅助信息）<br>$z$ $\rightarrow$ 噪音（生成器的输入数据）<br>$c$ $\rightarrow$ 潜在代码(latent code)<br>$p_c$ $\rightarrow$ 潜在代码的分布，可以为连续分布，也可以为离散分布<br>$p_x$ $\rightarrow$ 真实数据的分布<br>$p_{z}(z)$ $\rightarrow$ 原始噪音数据的分布<br>$p_g$ $\rightarrow$ 经过生成器后数据的分布<br>$G()$ $\rightarrow$ 生成映射函数（可微），结构为多层感知机，参数$\theta_{g}$<br>$D()$ $\rightarrow$ 判别映射函数（可微），结构为多层感知机，参数$\theta_{d}$<br>$Q(c|x)$ $\rightarrow$ 辅助分布用于逼近后验概率$P(c|x)$<br>$G(z,c;\theta_{g})$ $\rightarrow$ 将噪音$z$和潜在代码映射到新的数据空间<br>$D(x ; \theta_{d})$ $\rightarrow$ $x$来自真实数据而不是生成数据的概率（真=1，假=0）</p>
<h3><span id="153-直观感受">1.5.3. 直观感受</span></h3><p><img src="https://image.zkhweb.top/20200209213237.png" alt="20200209213237.png"><br>当Q与D共享参数<br><img src="https://image.zkhweb.top/20200209213913.png" alt="20200209213913.png"></p>
<h3><span id="154-目标函数">1.5.4. 目标函数</span></h3><script type="math/tex; mode=display">
\begin{aligned}
\min _{G} \max _{D} V_{I}(D, G)
&=V(D, G)-\lambda I(c ; G(z, c)) \\
&= { \mathbb{E} }_{ x ～ { p }_  { data } (x) }[logD(x)] + { \mathbb{E} }_{ z ～ { p }_{ z }(z) }[log(1-D(G(z)))]-\lambda I(c ; G(z, c))
\end{aligned}</script><p>和原始GAN相近，只是G中加入了潜码$c$（和$G(z,c)$有关），可以调节$c$来改变生成样式(加入了互信息的惩罚)</p>
<p>当用分布$Q(c|x)$逼近后验概率$P(c|x)$时，目标函数变为</p>
<script type="math/tex; mode=display">\min_{G,Q}\max_D V_{InfoGAN}(D,G,Q)=V(D,G)-\lambda L_I(G,Q)</script><p><strong>Note</strong>:此处的$V(D, G)$不局限于原始GAN的代价函数，可以使用其他的代价函数，以获得更好的训练</p>
<h3><span id="155-对抗">1.5.5. 对抗</span></h3><p>判别器$D$的目标</p>
<ol>
<li>要尽可能把真的样本判断为真，对应最大化第一项：${ E }_{ x ～ { p }_  { data } (x) }[logD(x)]$</li>
<li>把假的样本判断为假，对应最大化第二项：${ E }_{ z ～ { p }_{ z }(z) }[log(1-D(G(z,c)))] $</li>
</ol>
<ul>
<li>总之，也就是说<strong>判别器$D$要最大化目标函数</strong>；</li>
</ul>
<p>生成器$G$的目标</p>
<ol>
<li>要尽可能的让$D$将生成的假样本判断为真，对应最小化第二项：${ E }_{ z ～ { p }_{ z }(z) }[log(1-D(G(z)))] $</li>
<li>同时加入潜码$c$是为了使生成$G(z,c)$和$c$有很强的关联：$I(c ; G(z, c))$最大化，即$-\lambda I(c ; G(z, c))$($\lambda$为整数)最小</li>
</ol>
<ul>
<li>总之，也就是说<strong>生成器$G$要最小化目标函数</strong>；</li>
</ul>
<p>总的来说，这是一个<strong>信息正则化的MinMax Game</strong>；<br>当用分布$Q(c|x)$逼近后验概率$P(c|x)$时，InfoGAN<strong>具有互信息和超参数λ的变分正则化的MinMax Game</strong></p>
<h3><span id="156-目标函数由来">1.5.6. 目标函数由来</span></h3><p>为了让让网络学习到了可解释的特征表示<br>将提出将<strong>输入噪声向量分成为两部分</strong>：</p>
<ul>
<li>$z$:被视为不可压缩的噪声源；</li>
<li>$c$<sup><a href="#fn_1" id="reffn_1">1</a></sup>:将其称为潜在代码(latent code)，其目的在于数据分布的显著结构化的语义特征。</li>
</ul>
<p><sup><a href="#fn_1" id="reffn_1">1</a></sup>:在数学上，$c_1,c_2,…,c_L$​ 表示结构化潜在变量的集合。在其最简单的形式中，可以假设一个因式分布，由 $P\left(c_{1}, c_{2}, \ldots, c_{L}\right)=\prod_{i=1}^{L} P\left(c_{i}\right)$ 给出。为了便于表示，使用潜在代码 $c$ 来表示所有潜在变量 $c_i$​ 的联合。</p>
<p>但是在标准GAN中，通过找到满足 $P_G(x|c)= P_G(x)$ 的解，生成器可以自由地忽略附加潜在代码$c$ 。</p>
<p>提出了一种<strong>基于信息论的正则化方法</strong>：潜在码 $c$ 和生成分布 $G(z,c)$ 之间应该有很高的互信息。因此 $I(c;G(z,c))$ 应该很高。</p>
<p>给定任何 $x \sim P_{G}(x)$ ，希望 $P_G(c|x)$ 具有小的熵。换句话说，<strong>潜在码 $c$ 中的信息不应该在生成过程中丢失</strong>。在聚类的背景下，之前已经有过类似的互信息启发目标函数。因此，本文建议通过以下信息正则化的minimax游戏来解决问题：</p>
<script type="math/tex; mode=display">
\min _{G} \max _{D} V_{I}(D, G)=V(D, G)-\lambda I(c ; G(z, c))</script><h3><span id="157-变分互信息最大化">1.5.7. 变分互信息最大化</span></h3><p><strong>引理1</strong>对于随机变量$X$，$Y$和函数$f(x,y)$在适当的正则条件下$\mathbb{E}_{x \sim X, y \sim Y | x}[f(x, y)]=\mathbb{E}_{x \sim X, y \sim Y\left|x, x^{\prime} \sim X\right| y\left[f\left(x^{\prime}, y\right)\right]}$</p>
<p><strong>证明</strong>：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbb{E}_{x \sim X, y \sim Y | x}[f(x, y)] &=\int_{x} P(x) \int_{y} P(y | x) f(x, y) d y d x \\
&=\int_{x} \int_{y} P(x, y) f(x, y) d y d x \\
&=\int_{x} \int_{y} P(x, y) f(x, y) \int_{x^{\prime}} P\left(x^{\prime} | y\right) d x^{\prime} d y d x \\
&=\int_{x} P(x) \int_{y} P(y | x) \int_{x^{\prime}} P\left(x^{\prime} | y\right) f\left(x^{\prime}, y\right) d x^{\prime} d y d x \\
&=\mathbb{E}_{x \sim X, y \sim Y\left|x, x^{\prime} \sim X\right| y}\left[f\left(x^{\prime}, y\right)\right]
\end{aligned}</script><p><strong>变分信息最大化</strong><br>互信息项 $I(c;G(z,c))$ 难以直接最大化，因为它先得到后验概率 $P(c|x)$ 。<br><strong>通过定义辅助分布 $Q(c|x)$ 来逼近 $P(c|x)$</strong> ，可以得到它的下界：</p>
<script type="math/tex; mode=display">
\begin{aligned}
I(c ; G(z, c)) &=H(c)-H(c | G(z, c)) \\
&=\mathbb{E}_{c \sim P(c),x \sim G(z, c)}[\log P(c |x)]+H(c)\\
根据引理1&=\mathbb{E}_{x \sim G(z, c)} \left[\mathbb{E}_{c^{\prime} \sim P(c | x)} \left[\log P(c^{\prime}|x) \right] \right]+H(c)\\
&=\mathbb{E}_{x \sim G(z, c)}\left[\mathbb{E}_{c^{\prime} \sim P(c | x)}\left[\log P\left(c^{\prime} | x\right)\right]\right]+H(c) \\
&=\mathbb{E}_{x \sim G(z, c)}\left[\mathbb{E}_{c^{\prime} \sim P(c | x)} \left[ \frac{P(c^{\prime} | x) Q(c^{\prime} | x)}{Q(c^{\prime} | x)} \right] \right] \\
&=\mathbb{E}_{x \sim G(z, c)}\left[  \mathbb{E}_{c^{\prime} \sim P(c | x)} \left[ \log \frac{P(c^{\prime} | x)}{Q(c^{\prime} | x)}\right]+ \mathbb{E}_{c^{\prime} \sim P(c | x)}\left[\log Q\left(c^{\prime} | x\right)\right]\right]+H(c)\\
&=\mathbb{E}_{x \sim G(z, c)}\left[\underbrace{D_{K L}(P(\cdot | x) \| Q(\cdot | x))}_{\geq 0}+\mathbb{E}_{c^{\prime} \sim P(c | x)}\left[\log Q\left(c^{\prime} | x\right)\right]\right]+H(c) \\
& \geq \mathbb{E}_{x \sim G(z, c)}[{\left.D_{c^{\prime} \sim P(c | x)}\left[\log Q\left(c^{\prime} | x\right)\right]\right]}+H(c)
\end{aligned}</script><p>潜在编码 $H(c)$ 的熵也可以优化，因为对于常见分布，它具有简单的分析形式。然而，在本文中，通过修复潜在编码分布来选择简化的表示，并<strong>将 $H(c)$ 视为常量</strong>。</p>
<p><strong>引理2</strong> 对随机变量 $X$,$Y$ 和函数 $f(x,y)$ ，在合适的规则条件下： $\mathbb{E}_{x \sim X, y \sim Y | x}[f(x, y)]=\mathbb{E}_{x \sim X, y \sim Y|x, x^{\prime} \sim X| y}[f\left(x^{\prime}, y\right)]$</p>
<p><strong>证明</strong>：<br>通过使用引理1可以定义互信息 $I(c;G(z,c))$ 变分的下界 $L_I(G,Q)$:</p>
<script type="math/tex; mode=display">
\begin{aligned}
L_{I}(G, Q) &=E_{c \sim P(c), x \sim G(z, c)}[\log Q(c | x)]+H(c) \\
&=E_{x \sim G(z, c)}\left[\mathbb{E}_{c^{\prime} \sim P(c | x)}\left[\log Q\left(c^{\prime} | x\right)\right]\right]+H(c) \\
& \leq I(c ; G(z, c))
\end{aligned}</script><p>注意到 $L_I(G,Q)$ 很容易用<strong>蒙特卡罗模拟近似</strong>。 特别是，可以对于 w.r.t.$Q$ 和 w.r.t.$G$ 使用重参数化技巧将 $L_I$​ 最大化。 因此， $L_I(G,Q)$ 可以添加到GAN的目标而不改变GAN的训练过程，称之为<strong>信息最大化生成对抗网络(InfoGAN)</strong>。</p>
<p>变分互信息最大化表明，当辅助分布 $Q$ 接近真实的后验分布 $\mathbb E_x[D_{KL}P(·|x)||Q(·|x))]→0$ 时，下限变紧了。另外，当变分下界达到离散潜码的最大值 $L_I(G,Q)= H(c)$ 时，边界变紧并且达到最大互信息。</p>
<hr>
<h2><span id="16-实现">1.6. 实现</span></h2><p><strong>整体</strong></p>
<ul>
<li>将辅助分布 $Q$ 参数化为神经网络。 <ul>
<li>在大多数实验中， $Q$ 和 $D$ <strong>共享所有卷积层</strong></li>
<li>存在一个最终完全连接的层以输出条件分布 $Q(c|x)$ 的参数(softmax)</li>
</ul>
</li>
</ul>
<p>这意味着InfoGAN仅向GAN添加了<strong>可忽略的计算成本</strong><br>在实验中， $L_I(G,Q)$ 总是比正常的GAN目标更快收敛</p>
<p><strong>分类的潜在代码$c_i$​</strong></p>
<ul>
<li>使用非线性softmax来表示 $Q(c_i|x)$<ul>
<li>对于连续潜在代码 $c_j$​ ，根据真正的后验概率 $P(c_j|x)$ ，有更多的选项。可以简单地将 $Q(c_j|x)$ 作为因子化的高斯来处理就足够了。</li>
</ul>
</li>
<li><p><strong>c为离散变量</strong><br>$Q(c|x)$可以表示为一个神经网络$Q(X)$的输出</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\because L_{I}(G, Q)=c \cdot \log Q(G(z, c))+H(c)\\
&\therefore \max L_{I}(G, Q) \text{等价于} \max c \cdot \log Q(G(z, c))
\end{aligned}</script><p>其中$\cdot$表示内积，c是一个选择计算哪个$\log$的参数，$H(c)$可以消去<br><strong>$L_I(G,Q)$本质上是$x$与$G(z,x)$之间的KL散度</strong></p>
<script type="math/tex; mode=display">\begin{aligned}
&\because \begin{aligned}
D_{K L}(c \| Q(G(z, c))) &=-c \cdot \log \frac{Q(G(z, c))}{c} \\
&=-c \cdot \log Q(G(z, c))+c \cdot \log c \\
&=-c \cdot \log Q(G(z, c))+H(c) \\
&=-L_{I}(G, Q)+2 H(c)
\end{aligned}\\
&\therefore \max L_{I}(G, Q) \text{等价于} \min D_{K L}(c \| Q(G(z, c)))
\end{aligned}</script><p>$\min D_{K L}(c | Q(G(z, c)))$意味着减少$c$与$Q(G(z, c)$</p>
</li>
<li><p><strong>c为连续变量</strong><br>设 $Q(x)$ 输出的参数潜码 $c$ 的均值 $\mu$，标准差 $\sigma$ 分别为 $Q(x)_{\mu}$ 和$Q(x)_{\sigma}$，那么对于参数潜码 c</p>
<script type="math/tex; mode=display">
\begin{aligned}
&L_{I}(G, Q)=\mathbb{E}_{x \sim G(z, c)}\left[\mathbb{E}_{c^{\prime} \sim P(c | x)}\left[\log Q\left(c^{\prime} | x\right)\right]\right]+H(c) \\
&\because p(x)=\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}(\text{c符合正态分布;均值 $\mu$,标准差 $\sigma$})\\
&\therefore \log p(c) = -\frac{(c-\mu)^{2}}{2 \sigma^{2}}-\log (\sigma \sqrt{2 \pi})\\
&\therefore L_{I}(G, Q)=-\frac{\left(c-Q(x)_{\mu}\right)^{2}}{2 Q(x)_{\sigma}^{2}}-\log \left(Q(x)_{\sigma} \sqrt{2 \pi}\right)+H(c)\\
&\therefore \max L_{I}(G, Q) \text{等价于} \min \left(\frac{\left(c-Q(x)_{\mu}\right)^{2}}{2 Q(x)_{\sigma}^{2}}+\log \left(Q(x)_{\sigma} \sqrt{2 \pi}\right)\right)
\end{aligned}</script><p>不考虑$Q(x)_{\sigma}$的影响则</p>
<script type="math/tex; mode=display">
\max L_{I}(G, Q) \text{等价于} \min \left(c-Q(x)_{\mu}\right)^{2}</script><p>$\min \left(c-Q(x)_{\mu}\right)^{2}$意味这减小$c$与$Q(x)_{\mu}$的差</p>
</li>
<li><p><strong>总之$L_{I}$的优化过程，实质上是以G为编码器(Encoder)，Q为解码器(Decoder)，生成图像作为要编码的码(code)，训练一个自编码器(Autoencoder)</strong></p>
</li>
</ul>
<p><strong>超参数 $\lambda$</strong></p>
<ul>
<li>对于<em>离散</em>的潜在代码，它很容易调整，简单地设置为1就足够。</li>
<li>当潜在编码包含<em>连续</em>变量时，较小的 $\lambda$ 通常用于确保包含差分熵的 $L_I(G,Q)$ 的规模与GAN目标的规模相同。</li>
</ul>
<hr>
<h2><span id="17-实验">1.7. 实验</span></h2><p><strong>目标</strong></p>
<ul>
<li>调查是否可以有效地最大化互信息</li>
<li>评估InfoGAN是否可以通过利用生成器一次仅改变一个潜在因子来学习有区分度的可解释的表示，以评估改变这样的因素是否导致生成的图像中只有一种类型的语义变化。 </li>
</ul>
<h3><span id="171-互信息的最大化">1.7.1. 互信息的最大化</span></h3><p>为了评估潜在编码 $c$ 和生成的图像 $G(z,c)$ 之间的互信息是否可以通过提出的方法有效地最大化，作者在MNIST数据集上训练InfoGAN，对潜在编码 $c$ 进行统一的分类分布 $c\sim Cat(K=10,p=0.1)$ 。在 图1 中，下限 $L_I(G,Q)$ 被快速最大化为 $H(c)\approx 2.30$ ，这意味着下限 (4) 快速紧贴到到最大的互信息。</p>
<p>作为基准，当没有明确促使生成图像与潜在编码最大化的互信息时，作者还训练具有辅助分布 $Q$ 的常规GAN。由于作者使用神经网络对 $Q$ 进行参数化，假设 $Q$ 可以合理地近似真实的后验概率 $P(c|x)$ ，因此在常规GAN中潜在编码和生成图像之间几乎没有互信息。作者注意到，使用不同的神经网络架构，即使在实验中没有观察到这种情况，潜在代码和生成的图像之间可能存在更高的相互信息。这种比较是为了证明在常规GAN中，不能保证生成器能够利用潜在编码。</p>
<h3><span id="172-有区分度的表示">1.7.2. 有区分度的表示</span></h3><p><strong>离散的c捕捉离散变换，连续的c捕捉连续变换</strong><br><strong>MNIST</strong></p>
<ul>
<li>$c_1\sim Cat(K=10,p=0.1)$ 控制数字</li>
<li>$c_2 \sim Unif(-1,1)$​ 控制旋转数字</li>
<li>$c_3 \sim Unif(-1,1)$​ 控制宽度</li>
</ul>
<p>但$c_2,c_3$​还调整其他细节，如厚度或笔触样式<br>$c$处于$-2 \sim 2$之间仍有用，表明InfoGAN学习的潜在表示可泛化</p>
<p><strong>Faces</strong><br>$c_i\sim Unif(-1,1),1\leq i\leq 5$<br>表示为方位角（姿势），仰角和照明等潜在因素看做连续潜在变量。</p>
<p><strong>CelebA</strong><br>$c_1,c_2,c_3,c_4\sim (K=20,p=0.05)$和一个连续码 $c_5\sim Unif(-1,1)$<br>代表旋转的连续编码。使用单个连续代码连续插入不同宽度的类似椅子类型。</p>
<p><strong>SVHN</strong><br>利用四个10维分类变量和两个均匀连续变量作为潜在代码。</p>
<hr>
<h2><span id="18-结论">1.8. 结论</span></h2><ul>
<li>InfoGAN完全没有监督</li>
<li>在具有挑战性的数据集上学习可解释和有区分度的表示。</li>
<li>InfoGAN在GAN之上仅增加了可忽略不计的计算成本，并且易于训练。 </li>
<li>使用互信息表示的核心思想可以应用于其他方法，</li>
</ul>
<hr>
<h2><span id="19-附录解释为sleep-sleep算法">1.9. 附录——解释为“sleep-sleep”算法</span></h2><p>InfoGAN可以看作是Helmholtz机（wake-sleep算法）：</p>
<ul>
<li>$P_{G}(x | c)$是生成分布</li>
<li>$Q(c|x)$识别分布</li>
</ul>
<p>提出wake-sleep算法，通过执行wake阶段和sleep阶段更新Helmholtz机</p>
<ul>
<li>wake阶段：通过优化有关生成的变分下界$\log P_G(x)$来更新<script type="math/tex; mode=display">\max _{G} \mathbb{E}_{x \sim \operatorname{Data}, c \sim Q(c | x)}\left[\log P_{G}(x | c)\right]</script></li>
<li>sleep阶段：通过在当前生成分布中生成样本，而不是从实际数据分布中提取样本来更新辅助分布</li>
</ul>
<p>因此，当优化代理损失函数<sup><a href="#fn_2" id="reffn_2">2</a></sup>$L_I$（关于$Q$），更新步骤正是wake-sleep算法中的sleep过程。<br>InfoGAN与Wake-sleep不同之处在于优化$L_I$（关于$Q$）生成网络G在潜在代码$P(c)$整个先前分布中使用了潜在代码$c$。由于InfoGAN在sleep过程中更新了生成器，所以可以解释为sleep-sleep算法。</p>
<p>这种解释突出了InfoGAN与以前生成建模技术的区别：<br>明确鼓励生成器以潜在代码传达信息，并建议将相同原理应用于其他生成模型。</p>
<p><sup><a href="#fn_2" id="reffn_2">2</a></sup>:代理损失函数:当原本的loss function不便计算的时候，我们就会考虑使用surrogate loss function。</p>
<hr>
<h2><span id="110-参考资料">1.10. 参考资料</span></h2><p><a href="https://arxiv.org/abs/1606.03657" target="_blank" rel="noopener">Paper—-InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</a><br><a href="https://blog.csdn.net/hjimce/article/details/55657325" target="_blank" rel="noopener">CSDN—-深度学习（四十八）InfoGAN学习笔记</a><br><a href="https://zhuanlan.zhihu.com/p/58261928" target="_blank" rel="noopener">知乎—-InfoGAN解读</a><br><a href="https://www.inference.vc/infogan-variational-bound-on-mutual-information-twice/" target="_blank" rel="noopener">个人Blog—-InfoGAN: using the variational bound on mutual information (twice)</a><br><a href="https://www.jiqizhixin.com/articles/2018-10-29-21" target="_blank" rel="noopener">机器之心—-InfoGAN：一种无监督生成方法 | 经典论文复现</a><br><a href="http://www.pianshen.com/article/448358405/" target="_blank" rel="noopener">程序员大本营—-【GAN ZOO翻译系列】InfoGAN： Interpretable Representation Learning by Information Maximizing GAN</a></p>

                    </article>
                    
    <blockquote class="post-license">
        <p>
            <strong>本文作者&nbsp;:&nbsp;幻华(Zkher)</strong>
            <br>
            <strong>
            
                本文使用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">署名-非商业性使用-相同方式共享 4.0 国际 (CC BY-NC-SA 4.0)</a> 协议
            </strong>
            <br>
            <strong>本文链接&nbsp;:&nbsp;</strong><a href="http://www.zkhweb.top/2020/02/09/2016-InfoGAN-Interpretable-Representation-Learning-by-Information-Maximizing-Generative-Adversarial-Nets阅读笔记/">http://www.zkhweb.top/2020/02/09/2016-InfoGAN-Interpretable-Representation-Learning-by-Information-Maximizing-Generative-Adversarial-Nets阅读笔记/</a>
        </p>
    </blockquote>



    <blockquote id="date-expire-notification" class="post-expired-notify">本文最后更新于 <span id="date-expire-num"></span> 天前，文中所描述的信息可能已发生改变</blockquote>
    <script>
        var dateUpdate = Date.parse("2020-02-12");
        var nowDate = new Date();
        var a = nowDate.getTime();
        var b = a - dateUpdate;
        var daysUpdateExpire = Math.floor(b/(24*3600*1000));
        if (daysUpdateExpire >= 120) {
            document.getElementById('date-expire-num').innerHTML = daysUpdateExpire;
        } else {
            document.getElementById('date-expire-notification').style.display = 'none';
        }
    </script>


<p class="post-footer-info mb-0 pt-0">本文发表于&nbsp;<time datetime="2020-02-09T15:54:52.000Z" itemprop="datePublished">2020-02-09</time>

    , 最后修改于&nbsp;<time datetime="2020-02-12T15:21:53.901Z" itemprop="dateModified">2020-02-12</time>

</p>
<p class="post-footer-info mb-0 pt-2">

<span class="post-categories-list mt-2">


<a class="post-categories-list-item" href="/categories/深度学习论文阅读/">深度学习论文阅读</a>

</span>



<span class="post-tags-list mt-2">


<a class="post-tags-list-item" href="/tags/深度学习/">#&nbsp;深度学习</a>

<a class="post-tags-list-item" href="/tags/论文阅读/">#&nbsp;论文阅读</a>

</span>


</p>

                </div>
                
<div class="post-nav px-2 bg-gray">
<ul class="pagination">
    <!-- Prev Nav -->
    
        <li class="page-item page-prev">
            <a href="/2020/02/12/2016-Unsupervised-Representation-Learning-with-Deep-Convolutional-Generative-Adversarial-Networks阅读笔记/">
                <div class="page-item-title"><i class="icon icon-back" aria-hidden="true"></i></div>
                <div class="page-item-subtitle">DCGAN:Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks阅读笔记</div>
            </a>
        </li>
    

    <!-- Next Nav -->
    
        <li class="page-item page-next">
            <a href="/2020/02/07/2013-Maxout-Networks阅读笔记/">
                <div class="page-item-title"><i class="icon icon-forward" aria-hidden="true"></i></div>
                <div class="page-item-subtitle">Maxout Networks阅读笔记</div>
            </a>
        </li>
    
</ul>
</div>

                
                    <!-- # Comment # -->
                    
                
            </div>
        </div>
    </div>
</div>

            <!-- ### Footer ### -->
            <footer class="text-center">
    <!-- footer copyright -->
    <p class="footer-copyright mb-0">Copyright&nbsp;©&nbsp;<span data-year></span>
        <a class="footer-copyright-a" href="http://www.zkhweb.top">幻华&#39;s Blog</a>
    </p>
    <!-- footer custom text -->
    <p class="footer-text mb-0">
    
    </p>
    <!-- footer develop info -->
    <p class="footer-develop mb-0">
        
            
    <!-- Busuanzi User Views -->
    <span id="busuanzi_container_site_uv" hidden>
        <span></span>
        <span id="busuanzi_value_site_uv"></span>
        <span>Viewers</span>
        
            <span>|</span>
        
    </span>


    <!-- Busuanzi Post Views -->
    <span id="busuanzi_container_site_pv" hidden>
        <span></span>
        <span id="busuanzi_value_site_pv"></span>
        <span>Views</span>
        
    </span>


        
        
        Powered by&nbsp;<!--
         --><a href="https://hexo.io" target="_blank" class="footer-develop-a" rel="nofollow noopener noreferrer">Hexo</a><span class="footer-develop-divider"></span>Theme&nbsp;-&nbsp;<!--
         --><a href="https://github.com/SukkaW/hexo-theme-suka" target="_blank" class="footer-develop-a" rel="noopener">Suka</a>
    </p>
</footer>

        

        <!-- ### Import File ### -->
        <!-- ### Footer JS Import ### -->

<script>
window.lazyLoadOptions = {
    elements_selector: ".lazyload",
    threshold: 50
};



    /* Copyright */
    var copyrightNow = new Date().getFullYear();
    var copyrightContent = document.querySelector('span[data-year]')
    
        copyrightContent.textContent = copyrightNow
    



/* Cnosole Log */
console.log('\n %c Suka Theme (hexo-theme-suka) | © SukkaW | Verision 1.3.2 %c https://github.com/SukkaW/hexo-theme-suka \n', 'color: #fff; background: #444; padding:5px 0;', 'background: #bbb; padding:5px 0;');
</script>

<!-- vanilla-lazyload -->

    <script src="/lib/vanilla-lazyload/lazyload.min.js" async></script>



    <!-- Busuanzi -->
    
    <script src="https://cdn.jsdelivr.net/npm/busuanzi@2.3.0/bsz.pure.mini.js" async></script>


<!-- Offset -->







<!-- gallery.js -->


<!-- Comment -->


<!-- ### Custom Footer ### -->


    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>

</html>